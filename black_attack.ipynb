{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhiji/anaconda/envs/Fintech/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/zhiji/anaconda/envs/Fintech/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. load dataframe from cvs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/zhiji/Desktop/实习/ccts/黑产攻击识别/\"\n",
    "train = pd.read_csv(path + \"atec_anti_fraud_train.csv\")\n",
    "test = pd.read_csv(path + \"atec_anti_fraud_test_a.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 2. pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990006\n"
     ]
    }
   ],
   "source": [
    "#there is totally 297 features\n",
    "\n",
    "train_labeled_raw = train[train.label != -1]\n",
    "# drop all nan features\n",
    "train_labeled = train_labeled_raw.fillna(0)\n",
    "print(len(train_labeled_raw))\n",
    "features = []\n",
    "for c in train_labeled.columns:\n",
    "    if c[0] == 'f':\n",
    "        features.append(c)\n",
    "x = train_labeled[features]\n",
    "y = train_labeled.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas, Cs = [], []\n",
    "for a in np.r_[-15: 4][::2]:\n",
    "    gamma = pow(float(2), a)\n",
    "    gammas.append(gamma)\n",
    "for b in np.r_[-5: 16][::2]:\n",
    "    c = pow(float(2), b)\n",
    "    Cs.append(c)\n",
    "# Set the parameters by cross-validation  \n",
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': gammas,  'C': Cs}\n",
    "stat_svm = get_stat_SVM(x, y, tuned_parameters, split = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_distribution(label, classnum):\n",
    "  if (classnum == 2):\n",
    "    z, t = ZeroTwoFrequency(label)\n",
    "    return 'Up：' + str(z) + ' Down：' + str(t)\n",
    "  if (classnum == 3):\n",
    "    z, o, t = ZeroOneTwoFrequency(label)\n",
    "    return 'Up：' + str(z) + ' Flat: ' + str(o) + ' Down：' + str(t)\n",
    "def get_stat_SVM(X, y, tuned_parameters, split = 0.8):\n",
    "    d_test, d_train = [], []\n",
    "    a_train = []\n",
    "    a_test =  []\n",
    "    classnum = 2\n",
    "    #get cleaned technical indicators\n",
    "#     X, y = get_dataframe(name, iwl, fh, start_date, end_date,freq, classnum = classnum, method = 'SVM')\n",
    "    edge = np.int(split*len(y))\n",
    "    X_train, X_test, y_train, y_test = X[:edge], X[edge:],y[:edge],y[edge:]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_nomalized = scaler.transform(X_train)\n",
    "    X_test_nomalized = scaler.transform(X_test)\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv = 5,\n",
    "                  scoring = 'accuracy')\n",
    "    clf.fit(X_train_nomalized, y_train)\n",
    "    prediction_train = clf.predict(X_train_nomalized)\n",
    "    prediction_test = clf.predict(X_test_nomalized)\n",
    "    print(\"2 classes prediction: \", prediction_train, prediction_test)\n",
    "    a_train.append(accuracy_score(y_train, prediction_train))\n",
    "    a_test.append(accuracy_score(y_test, prediction_test))\n",
    "#     z_train, t_train = ZeroTwoFrequency(y_train)\n",
    "#     z_test, t_test = ZeroTwoFrequency(y_test)\n",
    "\n",
    "    d_train.append(svm_distribution(y_train, classnum))\n",
    "    d_test.append(svm_distribution(y_test, classnum))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "      '训练分布':d_train,\n",
    "      '测试分布':d_test,\n",
    "      '训练精度SVM': a_train,\n",
    "      '测试精度SVM': a_test\n",
    "    },index = idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ann(inputs,bs, epochs, neuron_num,verbose = False ):\n",
    "    layer_num = len(neuron_num)\n",
    "    nx = neuron_num[0]\n",
    "    ny = neuron_num[layer_num - 1]\n",
    "    weights, biases = {}, {}\n",
    "    x = tf.placeholder('float32',[None, nx])\n",
    "    y = tf.placeholder('float32',[None, ny])\n",
    "    layer_l = x\n",
    "    for l in range(layer_num - 2):\n",
    "        input_num = neuron_num[l]\n",
    "        hidden_num = neuron_num[l + 1]\n",
    "        weights['h' + str(l+1)] = tf.Variable(tf.random_normal([input_num, hidden_num]))\n",
    "        biases['b' + str(l+1)] = tf.Variable(tf.random_normal([hidden_num]))\n",
    "\n",
    "        layer_l = tf.nn.relu(tf.add(tf.matmul(layer_l, weights['h' + str(l+1)]), biases['b' + str(l+1)]))\n",
    "\n",
    "    weights['out'] = tf.Variable(tf.random_normal([hidden_num, ny]))\n",
    "    biases['out'] = tf.Variable(tf.random_normal([ny]))\n",
    "    prediction = tf.nn.softmax(tf.matmul(layer_l, weights['out']) + biases['out'])\n",
    "    tf.add_to_collection('pred', y)\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        losses, epoches = [],[]\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss=0\n",
    "            i=0\n",
    "            while i < len(X_nomalized):\n",
    "                start = i\n",
    "                end = i + bs\n",
    "                batch_x = np.array(X_nomalized[start:end])\n",
    "                batch_y = np.array(Y[start:end])\n",
    "\n",
    "                _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n",
    "                if verbose:\n",
    "                    epoch_loss+= c\n",
    "                i+= bs\n",
    "            if (verbose) :\n",
    "                if (epoch % 1000 == 0):\n",
    "                    print(\"Epoch\",epoch , 'completed out of ' ,epochs, ' loss: ', epoch_loss )\n",
    "                losses.append(epoch_loss)\n",
    "                epoches.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
       "       ...\n",
       "       'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296',\n",
       "       'f297'],\n",
       "      dtype='object', length=299)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_model_ANN(iwl, fh, name, bs, epochs, neuron_num, start_date, end_date,freq, verbose = False):\n",
    "  classnum = neuron_num[-1]\n",
    "  X, Y = get_dataframe(name, iwl, fh, start_date, end_date, freq, classnum = classnum, method = 'ANN')\n",
    "  scaler = preprocessing.StandardScaler().fit(X)\n",
    "  X_nomalized = scaler.transform(X)\n",
    "\n",
    "  layer_num = len(neuron_num)\n",
    "  nx = neuron_num[0]\n",
    "  ny = neuron_num[layer_num - 1]\n",
    "  weights, biases = {}, {}\n",
    "  #get nomalized train and test set\n",
    "\n",
    "\n",
    "  x = tf.placeholder('float32',[None, nx])\n",
    "  y = tf.placeholder('float32',[None, ny])\n",
    "  layer_l = x\n",
    "  for l in range(layer_num - 2):\n",
    "    input_num = neuron_num[l]\n",
    "    hidden_num = neuron_num[l + 1]\n",
    "    weights['h' + str(l+1)] = tf.Variable(tf.random_normal([input_num, hidden_num]))\n",
    "    biases['b' + str(l+1)] = tf.Variable(tf.random_normal([hidden_num]))\n",
    "\n",
    "    layer_l = tf.nn.relu(tf.add(tf.matmul(layer_l, weights['h' + str(l+1)]), biases['b' + str(l+1)]))\n",
    "\n",
    "  weights['out'] = tf.Variable(tf.random_normal([hidden_num, ny]))\n",
    "  biases['out'] = tf.Variable(tf.random_normal([ny]))\n",
    "  prediction = tf.nn.softmax(tf.matmul(layer_l, weights['out']) + biases['out'])\n",
    "  tf.add_to_collection('pred', y)\n",
    "\n",
    "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "  optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "      sess.run(tf.initialize_all_variables())\n",
    "      losses, epoches = [],[]\n",
    "      for epoch in range(epochs):\n",
    "          epoch_loss=0\n",
    "          i=0\n",
    "          while i < len(X_nomalized):\n",
    "              start = i\n",
    "              end = i + bs\n",
    "              batch_x = np.array(X_nomalized[start:end])\n",
    "              batch_y = np.array(Y[start:end])\n",
    "\n",
    "              _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n",
    "              if verbose:\n",
    "                  epoch_loss+= c\n",
    "              i+= bs\n",
    "          if (verbose) :\n",
    "                if (epoch % 1000 == 0):\n",
    "                  print(\"Epoch\",epoch , 'completed out of ' ,epochs, ' loss: ', epoch_loss )\n",
    "                losses.append(epoch_loss)\n",
    "                epoches.append(epoch)\n",
    "      path = \"./\"\n",
    "      model_name = \"ann\" + str(classnum) +\".ckpt\"\n",
    "      print(\"model name is : \" + model_name)\n",
    "      saver.save(sess, path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python(Fintech)",
   "language": "python",
   "name": "fintech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
