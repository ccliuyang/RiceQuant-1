{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhiji/anaconda/envs/Fintech/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,make_scorer,confusion_matrix,roc_curve, auc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. load dataframe from cvs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/zhiji/Desktop/实习/ccts/黑产攻击识别/\"\n",
    "train = pd.read_csv(path + \"atec_anti_fraud_train.csv\")\n",
    "test = pd.read_csv(path + \"atec_anti_fraud_test_a.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 2. pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990006\n"
     ]
    }
   ],
   "source": [
    "#there is totally 297 features\n",
    "\n",
    "train_labeled_raw = train[train.label != -1]\n",
    "# drop all nan features\n",
    "train_labeled = train_labeled_raw.fillna(0)\n",
    "print(len(train_labeled_raw))\n",
    "features = []\n",
    "for c in train_labeled.columns:\n",
    "    if c[0] == 'f':\n",
    "        features.append(c)\n",
    "x = train_labeled[features]\n",
    "y = train_labeled.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def svm_distribution(label, classnum):\n",
    "  if (classnum == 2):\n",
    "    z, t = ZeroTwoFrequency(label)\n",
    "    return 'Up：' + str(z) + ' Down：' + str(t)\n",
    "  if (classnum == 3):\n",
    "    z, o, t = ZeroOneTwoFrequency(label)\n",
    "    return 'Up：' + str(z) + ' Flat: ' + str(o) + ' Down：' + str(t)\n",
    "def get_stat_ocSVM(X, y,start_index, end_index, tuned_parameters, split = 0.8):\n",
    "    X = X[start_index:end_index]\n",
    "    y = y[start_index : end_index]\n",
    "    a_train = []\n",
    "    a_test =  []\n",
    "    classnum = 2\n",
    "    #get cleaned technical indicators\n",
    "    edge = np.int(split*len(y))\n",
    "    X_train, X_test, y_train, y_test = X[:edge], X[edge:],y[:edge],y[edge:]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_nomalized = scaler.transform(X_train)\n",
    "    X_test_nomalized = scaler.transform(X_test)\n",
    "\n",
    "    clf = GridSearchCV(OneClassSVM(), tuned_parameters, cv = 5,\n",
    "                  scoring = 'accuracy')\n",
    "    clf.fit(X_train_nomalized, y_train)\n",
    "    prediction_train = 0.5*(clf.predict(X_train_nomalized) + 1)\n",
    "    prediction_test = 0.5*(clf.predict(X_test_nomalized) + 1)\n",
    "    cm_train = confusion_matrix(y_train, prediction_train)\n",
    "    cm_test = confusion_matrix(y_test, prediction_test)\n",
    "    print(cm_train)\n",
    "    print(cm_test)\n",
    "    print(\"2 classes prediction: \", prediction_train, prediction_test)\n",
    "    a_train.append(accuracy_score(y_train, prediction_train))\n",
    "    a_test.append(accuracy_score(y_test, prediction_test))\n",
    "#     ba_train.append(black_accuracy(y_train, prediction_train))\n",
    "#     ba_test.append(black_accuracy(y_test, prediction_test))\n",
    "    return pd.DataFrame({\n",
    "        'confusion matrix train': cm_train,\n",
    "        'confusion matrix test': cm_test\n",
    "    },index = range(end_index - start_index))\n",
    "\n",
    "def get_stat_SVM(X, y, tuned_parameters, split = 0.8):\n",
    "    d_test, d_train = [], []\n",
    "    a_train = []\n",
    "    a_test =  []\n",
    "    classnum = 2\n",
    "    #get cleaned technical indicators\n",
    "#     X, y = get_dataframe(name, iwl, fh, start_date, end_date,freq, classnum = classnum, method = 'SVM')\n",
    "    edge = np.int(split*len(y))\n",
    "    X_train, X_test, y_train, y_test = X[:edge], X[edge:],y[:edge],y[edge:]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_nomalized = scaler.transform(X_train)\n",
    "    X_test_nomalized = scaler.transform(X_test)\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv = 5,\n",
    "                  scoring = 'accuracy')\n",
    "    clf.fit(X_train_nomalized, y_train)\n",
    "    prediction_train = clf.predict(X_train_nomalized)\n",
    "    prediction_test = clf.predict(X_test_nomalized)\n",
    "    print(\"2 classes prediction: \", prediction_train, prediction_test)\n",
    "    a_train.append(accuracy_score(y_train, prediction_train))\n",
    "    a_test.append(accuracy_score(y_test, prediction_test))\n",
    "#     z_train, t_train = ZeroTwoFrequency(y_train)\n",
    "#     z_test, t_test = ZeroTwoFrequency(y_test)\n",
    "\n",
    "    d_train.append(svm_distribution(y_train, classnum))\n",
    "    d_test.append(svm_distribution(y_test, classnum))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "      '训练分布':d_train,\n",
    "      '测试分布':d_test,\n",
    "      '训练精度SVM': a_train,\n",
    "      '测试精度SVM': a_test\n",
    "    },index = idx)\n",
    "def get_onehotcode(Y):\n",
    "    array = np.array(Y)\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.array_split(array, len(array)))\n",
    "    one_hot_code = enc.transform(np.array_split(array, len(array))).toarray()\n",
    "    return one_hot_code\n",
    "    \n",
    "def get_stat_ANN(X, y,start_index, end_index, bs, epochs, neuron_num, split = 0.8, verbose = False):\n",
    "    X = X[start_index : end_index]\n",
    "    y = y[start_index : end_index]\n",
    "    if not 1 in y:\n",
    "        raise error(\"there is only one class should be two, please enlarge the input range\")\n",
    "    y = get_onehotcode(y)\n",
    "    classnum = neuron_num[-1]\n",
    "    d_test, d_train = [], []\n",
    "    a_train = []\n",
    "    a_test = []\n",
    "    ba_train, ba_test = [], []\n",
    "    edge = np.int(split*len(y))\n",
    "    train_X, test_X, train_y, test_y = X[:edge], X[edge:],y[:edge],y[edge:]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_X)\n",
    "    X_train_nomalized = scaler.transform(train_X)\n",
    "    X_test_nomalized = scaler.transform(test_X)\n",
    "\n",
    "\n",
    "    layer_num = len(neuron_num)\n",
    "    nx = neuron_num[0]\n",
    "    ny = neuron_num[layer_num - 1]\n",
    "    weights, biases = {}, {}\n",
    "    #get nomalized train and test set\n",
    "\n",
    "\n",
    "    x = tf.placeholder('float32',[None, nx])\n",
    "    y = tf.placeholder('float32',[None, ny])\n",
    "    layer_l = x\n",
    "    for l in range(layer_num - 2):\n",
    "        input_num = neuron_num[l]\n",
    "        hidden_num = neuron_num[l + 1]\n",
    "        weights['w' + str(l+1)] = tf.Variable(tf.random_normal([input_num, hidden_num]))\n",
    "        biases['b' + str(l+1)] = tf.Variable(tf.random_normal([hidden_num]))\n",
    "        #       layer_l = tf.layers.dense(layer_l, hudden_num, activation = tf.nn.relu)\n",
    "        layer_l = tf.nn.relu(tf.add(tf.matmul(layer_l, weights['w' + str(l+1)]), biases['b' + str(l+1)]))\n",
    "\n",
    "    weights['out'] = tf.Variable(tf.random_normal([hidden_num, ny]))\n",
    "    biases['out'] = tf.Variable(tf.random_normal([ny]))\n",
    "    prediction = tf.nn.softmax(tf.matmul(layer_l, weights['out']) + biases['out'])\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "#     d_train.append(nn_distribution(train_y))\n",
    "#     d_test.append(nn_distribution(test_y))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        losses, epoches = [],[]\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss=0\n",
    "            i=0\n",
    "            while i < len(X_train_nomalized):\n",
    "                start = i\n",
    "                end = i + bs\n",
    "                batch_x = np.array(X_train_nomalized[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n",
    "                if verbose:\n",
    "                    epoch_loss+= c\n",
    "                i+= bs\n",
    "            if (verbose) :\n",
    "                  if (epoch % 1000 == 0):\n",
    "                        print(\"Epoch\",epoch , 'completed out of ' ,epochs, ' loss: ', epoch_loss )\n",
    "                  losses.append(epoch_loss)\n",
    "                  epoches.append(epoch)\n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        print(\"prediction: \",tf.argmax(prediction,1).eval({x:X_test_nomalized}))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "\n",
    "        a_test.append(accuracy.eval({x:X_test_nomalized , y: test_y}))\n",
    "        a_train.append(accuracy.eval({x:X_train_nomalized , y: train_y}))\n",
    "        ba_train.append(black_accuracy(tf.argmax(prediction,1).eval({x:X_train_nomalized}), train_y))\n",
    "        ba_test.append(black_accuracy(tf.argmax(prediction,1).eval({x:X_test_nomalized}), test_y))\n",
    "    return pd.DataFrame({\n",
    "    '训练预测攻击精度':ba_train,\n",
    "    '测试预测攻击精度':ba_test,\n",
    "    '训练精度ANN': a_train,\n",
    "    '测试精度ANN': a_test\n",
    "    })\n",
    "def black_accuracy(pred, label):\n",
    "    length = len(pred)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(length):\n",
    "        if pred[i] == 1 or label[i] == 1:\n",
    "            total += 1\n",
    "            if (pred[i] == label[i]):\n",
    "                correct += 1\n",
    "    return correct/total\n",
    "def black_detection(X, y,start_index, end_index, tuned_parameters, split = 0.8):\n",
    "    X = X[start_index : end_index]\n",
    "    y = y[start_index : end_index]\n",
    "    classnum = 2\n",
    "    \n",
    "    edge = np.int(split*len(y))\n",
    "    X_train, X_test, y_train, y_test = X[:edge], X[edge:],y[:edge],y[edge:]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_nomalized = scaler.transform(X_train)\n",
    "    X_test_nomalized = scaler.transform(X_test)\n",
    "    clf = GridSearchCV( OneClassSVM(), tuned_parameters, cv = 5,\n",
    "                  scoring = 'roc_auc')\n",
    "    clf.fit(X_train_nomalized, y_train)\n",
    "#     y_score = clf.fit(X_train_nomalized, y_train).decision_function(X_test_nomalized) \n",
    "    prediction_train = -0.5*clf.predict(X_train_nomalized) + 0.5\n",
    "    prediction_test = -0.5*clf.predict(X_test_nomalized) + 0.5\n",
    "    print(prediction_test)\n",
    "#     cm_train = confusion_matrix(y_train, prediction_train)\n",
    "#     cm_test = confusion_matrix(y_test, prediction_test)\n",
    "#     print(cm_train)\n",
    "#     print(cm_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, prediction_test)\n",
    "    print(fpr, tpr,thresholds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)' % (roc_auc))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gammas, Cs = [], []\n",
    "for a in np.r_[-15: 4][::2]:\n",
    "    gamma = pow(float(2), a)\n",
    "    gammas.append(gamma)\n",
    "for b in np.r_[-5: 16][::2]:\n",
    "    c = pow(float(2), b)\n",
    "    Cs.append(c)\n",
    "# Set the parameters by cross-validation  \n",
    "tuned_parameters = {'kernel': ['rbf'], 'gamma': gammas,  'C': Cs}\n",
    "tuned_parameters_oc = {'kernel': ['rbf'],'nu':[0.001,0.1,0.5], 'gamma': gammas}\n",
    "black_detection(x, y,1000,3000, tuned_parameters_oc, split = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python(Fintech)",
   "language": "python",
   "name": "fintech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
